# RAGAF-Diffusion: Dual-Stage Controllable Diffusion with Region-Adaptive Graph-Attention Fusion

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![GitHub stars](https://img.shields.io/github/stars/KumarSatyam24/Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion?style=social)](https://github.com/KumarSatyam24/Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion)

**Bridging Structure and Semantics: A Novel Diffusion Framework for Controllable Sketch-to-Image Generation**

</div>

---

## The Core Concept

**The Challenge:** How can we generate photorealistic images from sketches while preserving structural fidelity AND incorporating rich semantic details from text descriptions?

**Traditional Approaches Fall Short:**
- **Pure text-to-image**: Struggles with precise spatial control
- **Simple sketch conditioning**: Lacks semantic understanding of different regions
- **Uniform fusion**: Treats all image regions equally, missing context-specific requirements

**Our Innovation:** RAGAF-Diffusion treats sketch regions as a **semantic graph**, where each region can be intelligently fused with relevant text semantics through **graph attention**, enabling **region-aware**, **context-sensitive** generation.

---

## Conceptual Overview

### The Problem We Solve

Imagine you sketch a **house with a tree and a car**. You want:
- The **house** to be a "Victorian mansion"
- The **tree** to be a "cherry blossom in spring"
- The **car** to be a "vintage red sports car"

**Traditional methods** apply the entire text prompt uniformly across the image, leading to:
-  Semantic confusion (tree features bleeding into the house)
-  Structure-semantic mismatch (text details violating sketch structure)
-  Poor controllability (can't target specific regions)

**RAGAF-Diffusion** solves this by:
1. **Automatically detecting regions** in your sketch (house, tree, car)
2. **Building a spatial graph** of region relationships
3. **Using graph attention** to determine which text tokens are relevant to each region
4. **Adaptively fusing** sketch structure and text semantics based on the denoising timestep

### The RAGAF Philosophy

```
 Sketch Structure â†’  Region Graph â†’  Text Semantics â†’  Photorealistic Image
 (What + Where)      (Relationships)    (How + Details)      (Structure + Beauty)
```

**Three Core Principles:**

1. **Region-Awareness**: Different parts of a sketch have different semantic needs
2. **Adaptive Fusion**: Balance between structure and semantics evolves during generation
3. **Graph Reasoning**: Spatial relationships matter for coherent image synthesis

---

## Technical Innovation: The RAGAF Framework

### 1.  **Dual-Stage Architecture Design**

Our pipeline separates **structural generation** from **semantic refinement** for better controllability:

#### **Stage 1: Sketch-Guided Coarse Generation**
```
Purpose: Establish global structure and layout
Method:  ControlNet-style sketch conditioning
Output:  Structure-preserving coarse image
```

**Why separate stages?**
-  **Focus**: Each stage optimizes for one objective (structure vs. semantics)
-  **Flexibility**: Can use different guidance strengths for different generation goals
-  **Quality**: Prevents structure-semantic conflicts during generation

#### **Stage 2: RAGAF Semantic Refinement**
```
Purpose: Add semantic details while preserving structure
Method:  Region-adaptive graph attention fusion
Output:  Photorealistic image with rich details
```

**The key insight**: Structure and semantics have different importance at different denoising timesteps!

---

### 2. **Region-Adaptive Graph-Attention Fusion (RAGAF)**

This is the **core innovation** of our framework. Let's break it down:

#### **Step 1: Automatic Region Extraction** 

Instead of treating sketches as monolithic images, we decompose them into **meaningful regions**:

```python
# Conceptual process
sketch â†’ edge_detection â†’ connected_components â†’ regions
```

**Example:**
```
Input Sketch: [House with tree and car]

Detected Regions:
â”œâ”€ Region 1: House structure (center-top)
â”œâ”€ Region 2: Tree foliage (left)  
â”œâ”€ Region 3: Tree trunk (left-bottom)
â”œâ”€ Region 4: Car body (right)
â””â”€ Region 5: Background (scattered)
```

**Features per region** (6D vector):
-  Centroid location (x, y) - normalized
-  Area and perimeter
-  Bounding box dimensions
-  Shape compactness measure

**Why automatic?** No manual annotation required! Works with any sketch.

#### **Step 2: Graph Construction** 

Regions aren't isolated - they have **spatial relationships**:

```
Graph G = (V, E) where:
- V (nodes) = Sketch regions with spatial features
- E (edges) = Relationships between regions
```

**Edge Types:**
1. **Adjacency**: Regions that touch or overlap
2. **Proximity**: K-nearest neighbors by centroid distance
3. **Containment**: Nested regions (e.g., window inside house)

**Example Graph:**
```
     [House]
      /    \
[Window]  [Door]
     
   [Tree] â†â†’ [Ground]
     
   [Car]  â†â†’ [Road]
```

**Why graphs?** 
- Captures **spatial context** (tree is next to house)
- Enables **relational reasoning** (car should match road style)
- Models **part-whole relationships** (windows belong to house)

#### **Step 3: Graph Attention Mechanism** 

Not all region relationships are equally important! We use **multi-head graph attention**:

```python
# Simplified concept
for each region i:
    attention_weights = softmax(Q_i @ K_neighbors / âˆšd)
    updated_features_i = âˆ‘ attention_weights * V_neighbors
```

**What this does:**
-  Each region "attends" to relevant neighboring regions
-  Learns which relationships matter (e.g., roof relates to walls)
-  Propagates information across the graph

**Example:** When generating a "Victorian house":
- The **roof** region attends strongly to **wall** regions â†’ Maintains architectural consistency
- The **window** region attends to **house** region â†’ Ensures windows match house style
- The **tree** region has **weak attention** to house â†’ Can have independent style

#### **Step 4: Region-Text Cross-Attention** 

This is where **semantic control** happens:

```python
# For each region, compute attention with text tokens
attention_map[region_i, token_j] = relevance(region_i, token_j)
```

**The Magic:** Different text tokens influence different regions!

**Example with prompt: "A Victorian house with a cherry blossom tree"**

```
Text Token Attention Map:

"Victorian" â†’ High attention to [House, Roof, Window] regions
            â†’ Low attention to [Tree, Ground] regions

"cherry"    â†’ High attention to [Tree foliage] region
            â†’ Zero attention to [House] regions

"blossom"   â†’ High attention to [Tree foliage] region
            â†’ Low attention to [Tree trunk] region
```

**Why powerful?**
-  **Targeted semantics**: "Victorian" only affects the house
-  **No bleeding**: Tree style doesn't leak into house
-  **Fine control**: Different parts get different semantic guidance

#### **Step 5: Adaptive Fusion Weights** 

The **final innovation**: Fusion weights adapt based on **diffusion timestep**:

```python
Î±_sketch(t) = high when t is large  (early steps, noisy)
Î²_text(t)   = high when t is small  (late steps, denoised)

fused_features = Î±(t) * sketch_features + Î²(t) * text_features
```

**Intuition:**
- **Early timesteps** (t=1000 â†’ 700): Image is very noisy
  -  **Strong sketch guidance** (Î±=0.8): Establish correct structure
  -  Weak text guidance (Î²=0.2): Don't add details yet
  
- **Middle timesteps** (t=700 â†’ 300): Structure forming
  -  **Balanced guidance** (Î±=0.5, Î²=0.5): Refine both structure and semantics
  
- **Late timesteps** (t=300 â†’ 0): Near final image
  -  **Strong text guidance** (Î²=0.8): Add rich semantic details
  -  Weak sketch guidance (Î±=0.2): Allow flexibility for realism

**Why timestep-aware?**
-  **Structure first**: Get the layout right before adding details
-  **Details later**: Add texture, color, style when structure is stable
-  **Smooth transition**: Gradual shift from structure to semantics

---

### 3.  **Complete RAGAF Forward Pass**

Putting it all together:

```python
def RAGAF_forward(sketch, text_prompt, timestep_t):
    # 1. Extract regions from sketch
    regions = extract_regions(sketch)  # â†’ List of Region objects
    
    # 2. Build spatial graph
    graph = build_graph(regions)  # â†’ Graph G = (V, E)
    # V = node features (N, 6)
    # E = edge_index (2, num_edges)
    
    # 3. Graph attention over regions
    region_features = graph_attention(
        node_features=graph.V,
        edge_index=graph.E
    )  # â†’ (N, hidden_dim)
    # Each region's features updated with spatial context
    
    # 4. Text encoding
    text_embeddings = clip_encoder(text_prompt)  # â†’ (77, 768)
    
    # 5. Region-text cross-attention
    region_text_features = cross_attention(
        query=region_features,      # (N, hidden_dim)
        key_value=text_embeddings   # (77, 768)
    )  # â†’ (N, hidden_dim)
    # Each region gets relevant text semantics
    
    # 6. Adaptive fusion
    Î±, Î² = compute_fusion_weights(timestep_t, region_features)
    fused_features = Î± * sketch_features + Î² * region_text_features
    
    # 7. Inject into UNet for denoising
    denoised_latent = unet(noisy_latent, fused_features, timestep_t)
    
    return denoised_latent
```

**Information Flow:**
```
Sketch â†’ Regions â†’ Graph â†’ Spatial Context â†’ Text Relevance â†’ Adaptive Fusion â†’ Refined Image
  |         |        |            |                 |                 |              |
  |         |        |            |                 |                 |              â””â”€> Structure + Semantics
  |         |        |            |                 |                 â””â”€> Timestep-aware balance
  |         |        |            |                 â””â”€> Region-specific text influence
  |         |        |            â””â”€> Neighborhood awareness
  |         |        â””â”€> Spatial relationships
  |         â””â”€> Meaningful components
  â””â”€> User input
```

---

## ï¿½ Advantages Over Existing Methods

### Comparison with State-of-the-Art

| Capability | Stable Diffusion | ControlNet | Sketch-guided GAN | **RAGAF-Diffusion** |
|------------|------------------|------------|-------------------|---------------------|
| **Structural Control** | âŒ Weak | âœ… Strong | âœ… Strong | âœ… **Strong** |
| **Semantic Control** | âœ… Strong | âœ… Strong | âŒ Limited | âœ… **Strong** |
| **Region Awareness** | âŒ None | âŒ None | âŒ None | âœ… **Full** |
| **Adaptive Fusion** | âŒ No | âŒ No | âŒ No | âœ… **Yes** |
| **Spatial Reasoning** | âŒ No | âŒ No | âŒ No | âœ… **Graph-based** |
| **Timestep Awareness** | âš ï¸ Fixed | âš ï¸ Fixed | N/A | âœ… **Adaptive** |
| **Multi-region Text** | âŒ No | âŒ No | âŒ No | âœ… **Yes** |

### **Key Differentiators**

#### 1. **Region-Level Semantic Control** 
- **Others**: Apply text prompt uniformly to entire image
- **RAGAF**: Each region receives targeted semantic guidance
- **Benefit**: Prevents semantic bleeding, enables complex multi-object scenes

#### 2. **Graph-based Spatial Reasoning** 
- **Others**: Treat pixels/patches independently
- **RAGAF**: Model explicit spatial relationships via graph attention
- **Benefit**: Coherent multi-object layouts, context-aware generation

#### 3. **Adaptive Structure-Semantic Balance** 
- **Others**: Fixed fusion weights throughout generation
- **RAGAF**: Dynamic fusion adapts to denoising progress
- **Benefit**: Better structure preservation with rich semantic details

#### 4. **Automatic Region Discovery** 
- **Others**: Require manual masks or segmentation models
- **RAGAF**: Automatic region extraction from sketch
- **Benefit**: Zero manual annotation, works with any sketch

---

##  Theoretical Foundation

### **Problem Formulation**

Given:
- **Sketch** $S \in \mathbb{R}^{H \times W}$ (edge map)
- **Text prompt** $T$ (natural language description)

Goal: Generate image $I \in \mathbb{R}^{3 \times H \times W}$ that:
1. Preserves spatial structure from $S$
2. Incorporates semantic details from $T$
3. Is photorealistic and coherent

### **Mathematical Framework**

#### **1. Region Extraction**
```
R = {râ‚, râ‚‚, ..., râ‚™} = ConnectedComponents(S)
```
Where each region $r_i$ has features:
```
f_i = [x_i, y_i, area_i, perimeter_i, bbox_i, compactness_i] âˆˆ â„â¶
```

#### **2. Graph Construction**
```
G = (V, E) where:
- V = {fâ‚, fâ‚‚, ..., fâ‚™} (node features)
- E = {(i,j) | adjacency(ráµ¢, râ±¼) âˆ¨ proximity(ráµ¢, râ±¼)}
```

#### **3. Graph Attention**
```
h'áµ¢ = âˆ‘â±¼âˆˆN(i) Î±áµ¢â±¼ Â· Wáµ¥hâ±¼

where: Î±áµ¢â±¼ = softmax(eáµ¢â±¼)
       eáµ¢â±¼ = LeakyReLU(aáµ€[Wâ‚•háµ¢ || Wâ‚•hâ±¼])
```

#### **4. Region-Text Cross-Attention**
```
Attention(Q, K, V) = softmax(QKáµ€/âˆšd) Â· V

where: Q = region features (N Ã— d)
       K, V = text embeddings (77 Ã— 768)
```

#### **5. Adaptive Fusion**
```
Î±(t) = Ïƒ(w_Î± Â· Ï†(t) + b_Î±)  # Structure weight
Î²(t) = Ïƒ(w_Î² Â· Ï†(t) + b_Î²)  # Semantic weight

F_fused = Î±(t) âŠ™ F_sketch + Î²(t) âŠ™ F_text

where: Ï†(t) = timestep embedding
       Ïƒ = sigmoid activation
       âŠ™ = element-wise product
```

#### **6. Denoising Objective**
```
L = ğ”¼_t,xâ‚€,Îµ [||Îµ - Îµ_Î¸(x_t, F_fused, t)||Â²]

where: x_t = âˆšá¾±_t xâ‚€ + âˆš(1-á¾±_t) Îµ
       Îµ ~ N(0, I)
       Îµ_Î¸ = UNet denoiser
```

### **Why This Works**

1. **Graph structure** captures spatial dependencies â†’ Coherent layouts
2. **Cross-attention** aligns text with relevant regions â†’ Targeted semantics
3. **Adaptive fusion** balances objectives over time â†’ Structure + details
4. **Diffusion process** generates high-quality images â†’ Photorealism

---

## ğŸ—ï¸ Architecture Diagram

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT: Sketch + Text Prompt                       â”‚
â”‚              "A Victorian house with cherry blossom tree"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                 â”‚
     â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SKETCH ANALYSIS â”‚  â”‚  TEXT ENCODING   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Edge Detect   â”‚  â”‚ â€¢ CLIP Encoder   â”‚
â”‚ â€¢ Connected     â”‚  â”‚ â€¢ Token Embeddingsâ”‚
â”‚   Components    â”‚  â”‚   (77, 768)      â”‚
â”‚ â€¢ Extract N=15  â”‚  â”‚                  â”‚
â”‚   regions       â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â–¼                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REGION GRAPH CONSTRUCTIONâ”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚  Nodes: [House(râ‚), Roof(râ‚‚), Window(râ‚ƒ), ...,    â”‚
â”‚          Tree(râ‚‡), Trunk(râ‚ˆ), Car(râ‚â‚‚)]           â”‚
â”‚                                                     â”‚
â”‚  Edges: Houseâ†”Roof, Houseâ†”Window, Treeâ†”Trunk, ... â”‚
â”‚                                                     â”‚
â”‚  Features: (x, y, area, perimeter, bbox, compact.) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STAGE 1: SKETCH-GUIDED DIFFUSION             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Sketch Encoder (ControlNet-style)                   â”‚  â”‚
â”‚  â”‚  â€¢ Multi-scale feature extraction                   â”‚  â”‚
â”‚  â”‚  â€¢ Preserves edge information                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ UNet with Sketch Conditioning                       â”‚  â”‚
â”‚  â”‚  â€¢ Timestep t = 1000 â†’ 0                           â”‚  â”‚
â”‚  â”‚  â€¢ Denoising with sketch guidance                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â”‚                                       â”‚
â”‚                    â”‚ Coarse Image Iâ‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 2: RAGAF SEMANTIC REFINEMENT                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          RAGAF ATTENTION MODULE                      â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚ (A) GRAPH ATTENTION (Region Dependencies)   â”‚    â”‚ â”‚
â”‚  â”‚  â”‚                                              â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  For each region ráµ¢:                        â”‚    â”‚ â”‚
â”‚  â”‚  â”‚    Attend to neighbors N(i)                 â”‚    â”‚ â”‚
â”‚  â”‚  â”‚    Learn: Which regions influence which?    â”‚    â”‚ â”‚
â”‚  â”‚  â”‚                                              â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  Example:                                    â”‚    â”‚ â”‚
â”‚  â”‚  â”‚    â€¢ Roof(râ‚‚) â†’ [House(râ‚), Window(râ‚ƒ)]    â”‚    â”‚ â”‚
â”‚  â”‚  â”‚    â€¢ Tree(râ‚‡) â†’ [Trunk(râ‚ˆ), Ground(râ‚â‚€)]   â”‚    â”‚ â”‚
â”‚  â”‚  â”‚                                              â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  Output: Context-aware region features      â”‚    â”‚ â”‚
â”‚  â”‚  â”‚          h'áµ¢ = Î£â±¼ Î±áµ¢â±¼ Â· hâ±¼                   â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚                     â”‚                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚ (B) REGION-TEXT CROSS-ATTENTION              â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                               â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  For each region ráµ¢:                         â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    Attend to text tokens T = [tâ‚, ..., tâ‚‡â‚‡] â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    Learn: Which words apply to this region?  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                               â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  Example with "Victorian house, cherry tree":â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                               â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    House(râ‚)  â† "Victorian" (high attn)     â”‚   â”‚ â”‚
â”‚  â”‚  â”‚               â† "house" (high attn)          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚               â† "cherry" (zero attn)         â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                               â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    Tree(râ‚‡)   â† "Victorian" (zero attn)      â”‚   â”‚ â”‚
â”‚  â”‚  â”‚               â† "cherry" (high attn)         â”‚   â”‚ â”‚
â”‚  â”‚  â”‚               â† "tree" (high attn)           â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                               â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  Output: Text-enriched region features       â”‚   â”‚ â”‚
â”‚  â”‚  â”‚          z'áµ¢ = Attention(h'áµ¢, T, T)          â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚                     â”‚                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ (C) ADAPTIVE FUSION (Timestep-Aware)                 â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  Compute fusion weights based on timestep t:         â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  t=1000 (early, noisy):   Î±=0.8  Î²=0.2              â”‚ â”‚
â”‚  â”‚    â†’ Strong sketch, weak text                        â”‚ â”‚
â”‚  â”‚    â†’ Focus on structure                              â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  t=500 (middle):          Î±=0.5  Î²=0.5               â”‚ â”‚
â”‚  â”‚    â†’ Balanced                                        â”‚ â”‚
â”‚  â”‚    â†’ Refine both                                     â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  t=100 (late, clean):     Î±=0.2  Î²=0.8              â”‚ â”‚
â”‚  â”‚    â†’ Weak sketch, strong text                       â”‚ â”‚
â”‚  â”‚    â†’ Focus on semantic details                       â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  Fused = Î±(t) Â· Sketch + Î²(t) Â· Text-Region        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Inject into UNet for Refinement                      â”‚  â”‚
â”‚  â”‚  â€¢ Continue denoising with fused features            â”‚  â”‚
â”‚  â”‚  â€¢ Structure preserved, details enhanced             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   FINAL OUTPUT IMAGE    â”‚
              â”‚                         â”‚
              â”‚  âœ“ Structure preserved  â”‚
              â”‚  âœ“ Semantics applied    â”‚
              â”‚  âœ“ Photorealistic       â”‚
              â”‚  âœ“ Region-coherent      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### ï¿½ Component Details

#### **Region Extraction Pipeline**
```python
Input: Sketch S (HÃ—W grayscale)
â”‚
â”œâ”€> Edge Detection (if needed)
â”‚   â””â”€> Output: Binary edge map
â”‚
â”œâ”€> Connected Components Analysis
â”‚   â””â”€> Output: N labeled regions
â”‚
â”œâ”€> Feature Extraction per region:
â”‚   â”œâ”€> Centroid (xÌ„, È³)
â”‚   â”œâ”€> Area (pixel count)
â”‚   â”œâ”€> Perimeter (boundary length)
â”‚   â”œâ”€> Bounding box (x_min, y_min, width, height)
â”‚   â””â”€> Compactness (4Ï€Â·area/perimeterÂ²)
â”‚
â””â”€> Output: Region list R = [râ‚, râ‚‚, ..., râ‚™]
```

#### **Graph Construction Methods**

1. **Hybrid Graph** (default - best performance):
   ```python
   # Combine adjacency + KNN
   edges = adjacency_edges(regions) âˆª knn_edges(regions, k=5)
   ```

2. **Pure Adjacency**:
   ```python
   # Only touching/overlapping regions
   edges = {(i,j) | IoU(ráµ¢, râ±¼) > 0}
   ```

3. **KNN Graph**:
   ```python
   # K-nearest by centroid distance
   edges = {(i,j) | j âˆˆ KNN(centroid_i, k)}
   ```

#### **RAGAF Module Architecture**

```python
RAGAFAttentionModule(
    node_dim=6,              # Region features
    text_dim=768,            # CLIP embeddings
    hidden_dim=512,          # Internal representation
    num_graph_layers=2,      # Graph attention depth
    num_attention_heads=8,   # Multi-head attention
    dropout=0.1
)
# Total parameters: ~4.08M
```

---

##  Intuitive Examples: How RAGAF Works

### Example 1: "Victorian House with Cherry Blossom Tree"

```
INPUT SKETCH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    /\                  â”‚
â”‚   /  \      â•­â”€â•®        â”‚  â† House with triangular roof
â”‚  /____\     â”‚ â”‚        â”‚  â† Tree with foliage
â”‚  â”‚  â”‚ â”‚     â•°â”€â•¯        â”‚  â† Windows and trunk
â”‚  â”‚  â”‚ â”‚      â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

REGION EXTRACTION (Automatic):
râ‚: House body (center rectangle)
râ‚‚: Roof (top triangle)
râ‚ƒ: Left window
râ‚„: Right window
râ‚…: Door
râ‚†: Tree foliage (circle)
râ‚‡: Tree trunk (vertical line)

GRAPH STRUCTURE:
    râ‚‚(Roof)
       â†“
râ‚ƒâ”€â”€â†’ râ‚(House) â†â”€â”€râ‚„
   (Windows)
       â†“
    râ‚…(Door)
    
râ‚†(Foliage)
       â†“
    râ‚‡(Trunk)

TEXT PROMPT: "A Victorian mansion with a cherry blossom tree in spring"

REGION-TEXT ATTENTION MAP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Region   â”‚Victorian â”‚mansion  â”‚cherry  â”‚blossom â”‚spring   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚House(râ‚) â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚Roof(râ‚‚)  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚Window(râ‚ƒ)â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
â”‚Foliage(râ‚†)â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚Trunk(râ‚‡) â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â–ˆ = High attention  â–‘ = Low attention

RESULT:
 House receives "Victorian mansion" style â†’ Ornate architecture
 Tree receives "cherry blossom spring" details â†’ Pink flowers
 NO semantic bleeding â†’ Tree stays floral, house stays architectural
```

### Example 2: Adaptive Fusion Over Time

```
TEXT: "A vintage red sports car"

TIMESTEP t=1000 (Early - Very Noisy):
Î±(sketch) = 0.85, Î²(text) = 0.15
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚  Focus: Get car SHAPE right
â”‚ â–‘â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚  â†’ Sketch dominates
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚  â†’ Establish structure
â”‚ â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘         â”‚  Text influence: Minimal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIMESTEP t=500 (Middle - Partially Denoised):
Î±(sketch) = 0.5, Î²(text) = 0.5
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    â•”â•â•â•â•â•—               â”‚  Focus: Refine BOTH
â”‚ â•”â•â•â•¬â•â•â•â•â•¬â•â•â•—            â”‚  â†’ Balanced fusion
â”‚ â•‘  â•‘â—‹  â—‹â•‘  â•‘            â”‚  â†’ Structure + details
â”‚ â•šâ•â•â•â•â•â•â•â•â•â•â•            â”‚  Car shape + vintage hints
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIMESTEP t=100 (Late - Nearly Clean):
Î±(sketch) = 0.15, Î²(text) = 0.85
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    â•”â•â•â•â•â•—               â”‚  Focus: Semantic DETAILS
â”‚ â•”â•â•â•¬â•â•â•â•â•¬â•â•â•—            â”‚  â†’ Text dominates
â”‚ â•‘ğŸ”´â•‘ â—‰â—‰ â•‘ğŸ”´â•‘            â”‚  â†’ Add: Red color
â”‚ â•šâ•â•â•â•â•â•â•â•â•â•â•            â”‚  â†’ Add: Vintage chrome
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Add: Sports styling

FINAL OUTPUT:
 Structure preserved (car shape from sketch)
 Semantics applied (red, vintage, sports details)
 Photorealistic (smooth fusion)
```

### Example 3: Multi-Region Coherence

```
SKETCH: Living room scene

REGIONS:
râ‚: Sofa (left)
râ‚‚: Coffee table (center)
râ‚ƒ: Lamp (right)
râ‚„: Window (background)
râ‚…: Floor

TEXT: "A modern minimalist living room with natural wood furniture"

GRAPH ATTENTION BENEFITS:

Without Graph Attention (Independent):
âŒ Sofa: Modern style
âŒ Table: Random wood type (oak)
âŒ Lamp: Different metal (brass)
âŒ Overall: Incoherent mixture

With Graph Attention (RAGAF):
âœ… Sofa: Modern minimalist â†’ Influences neighbors
âœ… Table: Adopts same wood type (walnut) via attention to sofa
âœ… Lamp: Coordinates metal finish via attention to table
âœ… Floor: Harmonizes with furniture via global attention
âœ… Overall: Coherent, unified aesthetic

GRAPH ATTENTION MECHANISM:
    Window(râ‚„)
       â†“
Lamp(râ‚ƒ)  âŸ·  Sofa(râ‚)  âŸ·  Table(râ‚‚)
       â†˜       â†“       â†™
          Floor(râ‚…)

Each region "sees" its neighbors and adjusts its features for coherence!
```

---


##  Implementation & Practical Usage

### Sketchy Dataset (Primary)  **VALIDATED & READY**

<div align="center">

| Metric | Value |
|--------|-------|
|  **Total Pairs** | **75,481** |
|  **Categories** | **125 objects** |
|  **Train** | 52,514 samples (70%) |
|  **Validation** | 11,532 samples (15%) |
|  **Test** | 11,435 samples (15%) |
|  **Size** | ~10 GB |

</div>

**Download:** [https://sketchy.eye.gatech.edu/](https://sketchy.eye.gatech.edu/)

**Categories Include:** airplane, apple, bear, bicycle, cat, dog, elephant, guitar, horse, house, motorcycle, penguin, piano, rabbit, shoe, tree, and 109 more!

### MS COCO (Secondary - Optional)

- **Purpose**: Multi-object complex scenes
- **Size**: ~25 GB (images + annotations)
- **Train**: 118,287 images
- **Val**: 5,000 images
- **Features**: 5 captions per image, auto-generated sketches

**Download:** [https://cocodataset.org/](https://cocodataset.org/)

>  **Note**: You can train on **Sketchy only**. COCO is optional for multi-object experiments.

---

##  Quick Start

### âš¡ Installation (5 minutes)

```bash
# 1. Clone repository
git clone https://github.com/KumarSatyam24/Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion.git
cd Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion

# 2. Install dependencies
pip install -r requirements.txt

# 3. Verify installation
python verify_dataset.py  # If you have datasets downloaded
```

###  Dataset Setup (Optional - for training)

**Option 1: Automatic Setup (Sketchy)**
```bash
# Download Sketchy dataset from https://sketchy.eye.gatech.edu/
# Extract to your preferred location

# Set environment variable
export SKETCHY_ROOT=/path/to/sketchy
echo 'export SKETCHY_ROOT=/path/to/sketchy' >> ~/.zshrc

# Verify dataset
python check_sketchy_format.py /path/to/sketchy
```

**Option 2: Detailed Guide**

See **[DATASET_SETUP_GUIDE.md](DATASET_SETUP_GUIDE.md)** for comprehensive instructions including:
- Step-by-step download instructions
- Directory structure requirements
- Validation scripts
- Troubleshooting tips

###  Verify Setup

```bash
# Run comprehensive validation
python verify_dataset.py

# Expected output:
# âœ… SKETCHY_ROOT: /path/to/sketchy
# âœ… Dataset loaded: 52,514 training samples
# âœ… ALL CHECKS PASSED - READY FOR TRAINING!
```

---

##  Training

### System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| Python | 3.8+ | 3.10+ |
| CUDA | 11.8+ | 12.1+ |
| GPU Memory | 16GB | 24GB+ |
| GPU | RTX 3090 | RTX 4090 / A100 |
| RAM | 32GB | 64GB+ |
| Storage | 50GB | 100GB+ |

>  **Mac Users**: Training on CPU is extremely slow. Use cloud GPU (RunPod, Lambda Labs, AWS).

###  Training Commands

**Quick Start (Development):**
```bash
# Train both stages on Sketchy dataset
python train.py --dataset sketchy

# Train with subset for quick testing
python train.py \
    --dataset sketchy \
    --categories airplane,apple,bear,cat,dog \
    --epochs 2
```

**Full Training:**
```bash
# Stage 1: Sketch-guided diffusion
python train.py \
    --stage stage1 \
    --dataset sketchy \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --epochs 10 \
    --checkpoint_dir ./checkpoints/stage1

# Stage 2: Semantic refinement
python train.py \
    --stage stage2 \
    --dataset sketchy \
    --batch_size 4 \
    --learning_rate 5e-5 \
    --epochs 10 \
    --checkpoint_dir ./checkpoints/stage2

# Both stages (end-to-end)
python train.py \
    --stage both \
    --dataset sketchy \
    --batch_size 4 \
    --epochs 20
```

**Advanced Options:**
```bash
python train.py \
    --stage both \
    --dataset both \                    # Use both Sketchy and COCO
    --batch_size 8 \
    --gradient_accumulation_steps 2 \    # Effective batch size = 16
    --learning_rate 1e-4 \
    --mixed_precision fp16 \             # Memory efficient
    --use_lora \                         # LoRA fine-tuning
    --lora_rank 8 \
    --use_wandb \                        # Weights & Biases logging
    --wandb_project ragaf-diffusion \
    --seed 42
```

###  Cloud GPU Training (RunPod)

**Setup:**
```bash
# 1. Create RunPod account: https://runpod.io/
# 2. Select GPU: RTX 4090 or A100 recommended
# 3. SSH into instance

# 4. Clone and setup
git clone https://github.com/KumarSatyam24/Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion.git
cd Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion
pip install -r requirements.txt

# 5. Upload datasets to /workspace/datasets
# 6. Set environment variables
export SKETCHY_ROOT=/workspace/datasets/sketchy
export COCO_ROOT=/workspace/datasets/coco

# 7. Start training
python train.py \
    --stage both \
    --batch_size 8 \
    --mixed_precision fp16 \
    --checkpoint_dir /workspace/checkpoints \
    --use_wandb
```

**Expected Training Time:**

| Stage | Dataset | GPU | Epochs | Time |
|-------|---------|-----|--------|------|
| Stage 1 | Sketchy | RTX 4090 | 10 | ~6 hours |
| Stage 2 | Sketchy | RTX 4090 | 10 | ~8 hours |
| Both | Sketchy | RTX 4090 | 20 | ~14 hours |
| Both | Sketchy + COCO | A100 | 20 | ~24 hours |

---

##  Inference & Generation

### Basic Usage

```bash
python inference.py \
    --sketch examples/dog_sketch.png \
    --prompt "A photo of a golden retriever dog" \
    --stage1_checkpoint ./checkpoints/stage1/final.pt \
    --stage2_checkpoint ./checkpoints/stage2/final.pt \
    --output dog_output \
    --seed 42
```

### Advanced Options

```bash
python inference.py \
    --sketch my_sketch.png \
    --prompt "A beautiful sunset landscape with mountains" \
    --stage1_checkpoint checkpoints/stage1_best.pt \
    --stage2_checkpoint checkpoints/stage2_best.pt \
    --output landscape_output \
    --num_inference_steps 50 \          # More steps = higher quality
    --guidance_scale 7.5 \               # Classifier-free guidance
    --sketch_strength 0.8 \              # Sketch influence (0-1)
    --seed 42 \
    --save_intermediates                  # Save stage 1 output
```

### Output Structure

```
outputs/dog_output/
â”œâ”€â”€ sketch.png              # Input sketch (normalized)
â”œâ”€â”€ regions.png             # Extracted regions visualization
â”œâ”€â”€ region_graph.png        # Graph structure visualization
â”œâ”€â”€ stage1_output.png       # Stage 1 coarse output
â”œâ”€â”€ stage2_output.png       # Stage 2 refined output (final)
â”œâ”€â”€ comparison.png          # Side-by-side comparison
â”œâ”€â”€ attention_maps.png      # Region-text attention visualization
â””â”€â”€ metadata.json           # Generation parameters
```

### Batch Inference

```bash
# Generate from multiple sketches
python inference.py \
    --sketch_dir examples/sketches/ \
    --prompts_file examples/prompts.txt \
    --output_dir batch_outputs/ \
    --batch_size 4
```

---

##  Project Structure

```
Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion/
â”‚
â”œâ”€â”€  README.md                        # This file
â”œâ”€â”€  requirements.txt                 # Python dependencies
â”œâ”€â”€  DATASET_SETUP_GUIDE.md          # Detailed dataset instructions
â”œâ”€â”€  DEVELOPMENT.md                   # Developer documentation
â”œâ”€â”€  IMPLEMENTATION_SUMMARY.md        # Code organization details
â”‚
â”œâ”€â”€  data/                            # Data processing
â”‚   â”œâ”€â”€ sketch_extraction.py           # Edge detection (Canny, XDoG, HED)
â”‚   â”œâ”€â”€ region_extraction.py           # Connected component analysis
â”‚   â”œâ”€â”€ region_graph.py                # Spatial graph construction
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€  datasets/                        # Dataset loaders
â”‚   â”œâ”€â”€ sketchy_dataset.py             # Sketchy dataset (75k pairs)
â”‚   â”œâ”€â”€ coco_dataset.py                # MS COCO dataset
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€  models/                          # Core models
â”‚   â”œâ”€â”€ ragaf_attention.py             # RAGAF module (4.08M params)
â”‚   â”œâ”€â”€ adaptive_fusion.py             # Timestep-aware fusion
â”‚   â”œâ”€â”€ stage1_diffusion.py            # Sketch-guided diffusion
â”‚   â”œâ”€â”€ stage2_refinement.py           # Semantic refinement
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€  configs/                         # Configurations
â”‚   â”œâ”€â”€ config.py                      # Training/inference configs
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€  utils/                           # Utilities
â”‚   â”œâ”€â”€ common.py                      # Helper functions
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€  train.py                         # Main training script
â”œâ”€â”€  inference.py                     # Inference script
â”œâ”€â”€  verify_dataset.py                # Dataset validation
â””â”€â”€  check_sketchy_format.py          # Format checker
```



##  Examples

### Example 1: Simple Object
```bash
python inference.py \
    --sketch examples/apple_sketch.png \
    --prompt "A photo of a red apple on a wooden table" \
    --output apple_result
```

**Input Sketch** â†’ **Stage 1 (Structure)** â†’ **Stage 2 (Refined)**
```
   â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”
   â”‚ â—‹â—‹  â”‚   â”€â”€â”€â–º   â”‚ Grayâ”‚      â”€â”€â”€â–º    â”‚Photoâ”‚
   â”‚â—‹  â—‹ â”‚          â”‚Appleâ”‚              â”‚Appleâ”‚
   â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”˜
```

### Example 2: Animal with Details
```bash
python inference.py \
    --sketch examples/dog_sketch.png \
    --prompt "A golden retriever dog sitting on grass in a park" \
    --guidance_scale 8.0 \
    --sketch_strength 0.75
```

### Example 3: Complex Scene
```bash
python inference.py \
    --sketch examples/landscape_sketch.png \
    --prompt "A beautiful sunset over mountains with a lake in the foreground" \
    --num_inference_steps 100 \
    --sketch_strength 0.6
```

### Example 4: Multiple Variations
```bash
# Generate 5 variations from the same sketch
for seed in {1..5}; do
    python inference.py \
        --sketch examples/cat_sketch.png \
        --prompt "A fluffy white cat with blue eyes" \
        --output cat_var_$seed \
        --seed $seed
done
```

---

##  Configuration

### Default Configuration

Key hyperparameters in `configs/config.py`:

```python
# Model
pretrained_model_name = "runwayml/stable-diffusion-v1-5"
hidden_dim = 512
num_graph_layers = 2
num_attention_heads = 8

# Training
learning_rate = 1e-4
batch_size = 4
stage1_epochs = 10
stage2_epochs = 10
mixed_precision = "fp16"

# Fusion
fusion_method = "learned"  # or "heuristic", "hybrid"
use_region_adaptive_fusion = True

# LoRA (efficient fine-tuning)
use_lora = True
lora_rank = 4
```

### Custom Configuration

**Method 1: Command Line**
```bash
python train.py \
    --learning_rate 5e-5 \
    --batch_size 8 \
    --hidden_dim 768 \
    --num_graph_layers 3
```

**Method 2: YAML File**
```yaml
# config_custom.yaml
model:
  hidden_dim: 768
  num_graph_layers: 3
  fusion_method: "hybrid"

training:
  learning_rate: 5e-5
  batch_size: 8
  stage1_epochs: 15
  stage2_epochs: 15
```

```bash
python train.py --config config_custom.yaml
```

---

## Memory Optimization

For limited GPU memory:

```bash
# Use smaller batch size
--batch_size 2

# Use gradient accumulation
--gradient_accumulation_steps 4

# Use mixed precision
--mixed_precision fp16

# Freeze base UNet (train only RAGAF components)
--freeze_base_unet
```

## ğŸ“ˆ Monitoring & Logging

### Weights & Biases (Recommended)

```bash
# Enable W&B logging
python train.py --use_wandb --wandb_project ragaf-diffusion --wandb_run_name exp_001

# Tracked metrics:
# â€¢ Training/validation loss (stage 1 & 2)
# â€¢ Learning rate schedule
# â€¢ Fusion weights (sketch Î± vs text Î²)
# â€¢ Region-text attention maps
# â€¢ Generated sample images
# â€¢ GPU memory usage
# â€¢ Training speed (samples/sec)
```

**Dashboard Features:**
- ğŸ“Š Real-time loss curves
- ğŸ–¼ï¸ Generated image samples every N epochs
- ğŸ¯ Attention map visualizations
- âš¡ Training speed metrics
- ğŸ’¾ Automatic model versioning

### TensorBoard

```bash
# Launch TensorBoard
tensorboard --logdir ./checkpoints --port 6006

# View at http://localhost:6006

# Logged data:
# â€¢ Scalars: Loss, LR, fusion weights
# â€¢ Images: Generated samples, attention maps
# â€¢ Histograms: Model gradients, activations
# â€¢ Graphs: Model architecture
```

### Console Logging

```bash
# Training progress display:
Epoch 1/10 [Stage 1] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:05:23
â”œâ”€â”€ Train Loss: 0.1234
â”œâ”€â”€ Val Loss: 0.1567
â”œâ”€â”€ LR: 1.0e-04
â”œâ”€â”€ Samples/sec: 12.5
â””â”€â”€ ETA: 4h 32m

Epoch 1/10 [Stage 2] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:06:45
â”œâ”€â”€ Train Loss: 0.0987
â”œâ”€â”€ Val Loss: 0.1123
â”œâ”€â”€ Sketch Weight (Î±): 0.35
â”œâ”€â”€ Text Weight (Î²): 0.65
â””â”€â”€ Attention Entropy: 2.45
```

---

##  Evaluation

### Metrics (Coming Soon)

We will provide evaluation scripts for:

**Image Quality:**
- **FID** (FrÃ©chet Inception Distance) - Overall image quality
- **IS** (Inception Score) - Image diversity and quality
- **LPIPS** - Perceptual similarity

**Sketch Fidelity:**
- **Chamfer Distance** - Edge alignment with input sketch
- **IoU** - Region overlap with sketch regions
- **SSIM** - Structural similarity

**Text Alignment:**
- **CLIP Score** - Text-image semantic alignment
- **BERT Score** - Caption quality

**RAGAF-Specific:**
- **Attention Accuracy** - Region-text attention alignment
- **Fusion Balance** - Sketch vs text weight distribution
- **Graph Quality** - Region graph connectivity metrics

### Running Evaluation

```bash
# Coming soon
python evaluate.py \
    --checkpoint checkpoints/best.pt \
    --test_split test \
    --metrics fid,clip,chamfer \
    --output_dir evaluation_results/
```

---

### ğŸ’¬ Getting Help

1. **Check Documentation:**
   - [DEVELOPMENT.md](DEVELOPMENT.md) - Architecture details
   - [DATASET_SETUP_GUIDE.md](DATASET_SETUP_GUIDE.md) - Dataset help

2. **Run Validation:**
   ```bash
   python verify_dataset.py
   ```

3. **GitHub Issues:**
   - Search existing issues
   - Create new issue with error logs

4. **Debug Mode:**
   ```bash
   python train.py --debug --verbose
   ```

---

### Related Work

**Diffusion Models:**
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (Ho et al., NeurIPS 2020)
- [Stable Diffusion](https://arxiv.org/abs/2112.10752) (Rombach et al., CVPR 2022)

**Controllable Generation:**
- [ControlNet](https://arxiv.org/abs/2302.05543) (Zhang et al., ICCV 2023)
- [T2I-Adapter](https://arxiv.org/abs/2302.08453) (Mou et al., 2023)

**Graph Attention:**
- [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (VeliÄkoviÄ‡ et al., ICLR 2018)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., NeurIPS 2017)

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### Third-Party Licenses

- **Stable Diffusion**: CreativeML Open RAIL-M License
- **HuggingFace Transformers**: Apache License 2.0
- **PyTorch**: BSD License
- **Sketchy Dataset**: Academic use only
- **MS COCO**: Creative Commons Attribution 4.0

---

##  Acknowledgments

This project builds upon excellent prior work:

- **[Stable Diffusion](https://github.com/CompVis/stable-diffusion)** by CompVis - Base diffusion model architecture
- **[HuggingFace Diffusers](https://github.com/huggingface/diffusers)** - Diffusion model framework and utilities
- **[ControlNet](https://github.com/lllyasviel/ControlNet)** by Lvmin Zhang - Inspiration for sketch conditioning
- **[Sketchy Dataset](https://sketchy.eye.gatech.edu/)** by Georgia Tech - Sketch-photo paired dataset
- **[MS COCO](https://cocodataset.org/)** - Image-caption dataset
- **PyTorch Team** - Deep learning framework

---


### Development Setup

```bash
# 1. Fork and clone
git clone https://github.com/YOUR_USERNAME/Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion.git
cd Dual-Stage-Controllable-Diffusion-with-Adaptive-Modality-Fusion

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dev dependencies
pip install -r requirements.txt
pip install black flake8 pytest

# 4. Run tests
pytest tests/

# 5. Format code
black .
flake8 .
```

---

<div align="center">


**Made with â¤ï¸ by [Satyam Kumar](https://github.com/KumarSatyam24)**

</div>
